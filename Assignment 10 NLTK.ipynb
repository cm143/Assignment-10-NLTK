{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import nltk as nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem import porter\n",
    "from nltk.stem import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "os.path.join(os.path.dirname('RJ_Lovecraft.txt'))\n",
    "os.path.join(os.path.dirname('RJ_Tolkein.txt'))\n",
    "os.path.join(os.path.dirname('RJ_Martin.txt'))\n",
    "os.path.join(os.path.dirname('Martin.txt'))\n",
    "\n",
    "with open('RJ_Lovecraft.txt') as Lovecraft:\n",
    "    lovecraft_text = Lovecraft.readlines()\n",
    "    Lovecraft.close\n",
    "\n",
    "with open('RJ_Tolkein.txt') as Tolkein:\n",
    "    tolkein_text = Tolkein.readlines()\n",
    "    Tolkein.close\n",
    "\n",
    "with open('RJ_Martin.txt') as RJ_Martin:\n",
    "    rjmartin_text = RJ_Martin.readlines()\n",
    "    RJ_Martin.close\n",
    "\n",
    "with open('Martin.txt') as Martin:\n",
    "    martin_text = Martin.readlines()\n",
    "    Martin.close\n",
    "\n",
    "#tokenize sentences\n",
    "nltk.download('punkt')\n",
    "lovecraft_str = ''\n",
    "for i in lovecraft_text:\n",
    "    lovecraft_str += i\n",
    "tokenize.sent_tokenize(lovecraft_str)\n",
    "tolkein_str = ''\n",
    "for i in tolkein_text:\n",
    "    tolkein_str += i\n",
    "tokenize.sent_tokenize(tolkein_str)\n",
    "rjmartin_str = ''\n",
    "for i in rjmartin_text:\n",
    "    rjmartin_str += i\n",
    "tokenize.sent_tokenize(rjmartin_str)\n",
    "martin_str = ''\n",
    "for i in martin_text:\n",
    "    martin_str += i\n",
    "tokenize.sent_tokenize(martin_str)\n",
    "#tokenize words\n",
    "lovecraft_str = ''\n",
    "for i in lovecraft_text:\n",
    "    lovecraft_str += i\n",
    "lovecraft_words = tokenize.word_tokenize(lovecraft_str)\n",
    "tolkein_str = ''\n",
    "for i in tolkein_text:\n",
    "    tolkein_str += i\n",
    "tolkein_words = tokenize.word_tokenize(tolkein_str)\n",
    "rjmartin_str = ''\n",
    "for i in rjmartin_text:\n",
    "    rjmartin_str += i\n",
    "rjmartin_words = tokenize.word_tokenize(rjmartin_str)\n",
    "for i in martin_text:\n",
    "    martin_str += i\n",
    "martin_words = tokenize.word_tokenize(martin_str)\n",
    "#filtering out stop words\n",
    "nltk.download('stopwords') \n",
    "stop_words = corpus.stopwords.words('english')\n",
    "for i in stop_words:\n",
    "    while (i in lovecraft_words):\n",
    "       lovecraft_words.remove(i)\n",
    "\n",
    "for i in stop_words:\n",
    "    while (i in tolkein_words):\n",
    "       tolkein_words.remove(i)\n",
    "\n",
    "for i in stop_words:\n",
    "    while (i in rjmartin_words):\n",
    "       rjmartin_words.remove(i)\n",
    "\n",
    "for i in stop_words:\n",
    "    while (i in martin_words):\n",
    "       martin_words.remove(i)\n",
    "#stemming\n",
    "lovecraft_stem = porter.PorterStemmer()\n",
    "stemmed_lovecraft = []\n",
    "for i in lovecraft_words:\n",
    "    stemmed_lovecraft.append(lovecraft_stem.stem(i))\n",
    "   \n",
    "tolkein_stem = porter.PorterStemmer()\n",
    "stemmed_tolkein = []\n",
    "for i in tolkein_words:\n",
    "    stemmed_tolkein.append(tolkein_stem.stem(i))\n",
    "\n",
    "rjmartin_stem = porter.PorterStemmer()\n",
    "stemmed_rjmartin = []\n",
    "for i in rjmartin_words:\n",
    "    stemmed_rjmartin.append(rjmartin_stem.stem(i))\n",
    "\n",
    "#lemmatization\n",
    "lemnatized_lovecraft = stemmed_lovecraft\n",
    "for i in lemnatized_lovecraft:\n",
    "    lemnatized_lovecraft.append(lemnatized_lovecraft.lemnatize(i))\n",
    "\n",
    "lemnatized_tolkein = stemmed_tolkein\n",
    "for i in lemnatized_tolkein:\n",
    "    lemnatized_tolkein.append(lemnatized_tolkein.lemnatize(i))\n",
    "\n",
    "lemnatized_rjmartin = stemmed_rjmartin\n",
    "for i in lemnatized_rjmartin:\n",
    "    lemnatized_rjmartin.append(lemnatized_rjmartin.lemnatize(i))\n",
    "#Determine the 20 most common tokens in each text.\n",
    "lovecraft_freq = FreqDist(lovecraft_words)\n",
    "tolkein_freq = FreqDist(tolkein_words)\n",
    "rjmartin_freq = FreqDist(rjmartin_words)\n",
    "print(lovecraft_freq.pformat(maxlen = 20))\n",
    "print(tolkein_freq.pformat(maxlen = 20))\n",
    "print(rjmartin_freq.pformat(maxlen = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine how many named entities are contained in each text.\n",
    "from nltk.chunk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "lovecraft_tags = pos_tag(lovecraft_words)\n",
    "tolkein_tags = pos_tag(tolkein_words)\n",
    "rjmartin_tags = pos_tag(rjmartin_words)\n",
    "lovecraft_chunked = nltk.chunk.ne_chunk(lovecraft_tags)\n",
    "lovecraft_chunked.draw()\n",
    "tolkein_chunked = nltk.chunk.ne_chunk(tolkein_tags)\n",
    "tolkein_chunked.draw()\n",
    "rjmartin_chunked = nltk.chunk.ne_chunk(rjmartin_tags)\n",
    "rjmartin_chunked.draw()\n",
    "#Using both of these pieces of information, determine the subject of all three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-grams\n",
    "lovecraft_bigrams = nltk.ngrams(lovecraft_words, 3)\n",
    "tolkein_bigrams = nltk.ngrams(tolkein_words, 3)\n",
    "rjmartin_bigrams = nltk.ngrams(rjmartin_words, 3)\n",
    "martin_bigrams = nltk.ngrams(martin_words, 3)\n",
    "print(lovecraft_bigrams)\n",
    "print(tolkein_bigrams)\n",
    "print(rjmartin_bigrams)\n",
    "print(martin_bigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
