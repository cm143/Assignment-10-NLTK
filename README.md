The purpose of this assignment was to perform comparative analysis on three texts and n-gram analysis on the fourth text. I used .sent_tokenize to tokenize the sentences in each text, .word_tokenize to tokenize the words in each text, .pformat to find the 20 most common tokens in each text, .chunk.ne_chunk and .draw to determine how many named entities were in each text, and .ngrams to find the n-grams in the texts. I did not include any limitations.
